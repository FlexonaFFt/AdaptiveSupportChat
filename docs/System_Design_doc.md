# System Design Doc - AdaptiveSupport [RU]
## Дизайн ML/LLM системы - AdaptiveSupport MVP v1

### 1. Цели и предпосылки
#### 1.1. Зачем идем в разработку продукта?
- Бизнес-цель: ускорить первичную обработку обращений и снизить долю типовых вопросов, передаваемых живому оператору.
- Ожидаемый эффект: бот закрывает FAQ- и policy-вопросы на первой линии, оператор подключается только при низкой уверенности или нестандартных кейсах.
- Критерий успеха итерации MVP: стабильная работа Telegram-канала, корректные ответы по базе знаний и контролируемая эскалация на оператора.

#### 1.2. Бизнес-требования и ограничения
- Канал MVP: Telegram.
- Типы запросов: информационная поддержка и навигация по частым вопросам.
- Ограничение MVP: без CRM-интеграции и без автоматических действий над заказами.
- Ответ должен быть кратким, безопасным для пользователя и без раскрытия внутренних технических деталей (RAG/контекст/источники).

#### 1.3. Скоуп и анти-скоуп
- Входит в скоуп:
  - прием сообщений пользователя через webhook/polling;
  - retrieval по локальной базе знаний (`core/knowledge`);
  - генерация ответа через LLM (OpenAI-compatible или GigaChat);
  - fallback на оператора при низкой релевантности/неопределенности.
- Не входит в скоуп:
  - мультиканальность (web, email, voice);
  - персонализация на основе профиля клиента;
  - аналитический контур с A/B и продуктовой BI-дашбордизацией.

#### 1.4. Предпосылки решения
- База знаний хранится в markdown/text и регулярно актуализируется бизнес-командой.
- Для MVP достаточно легкого локального retriever (TF-IDF) без внешней векторной БД.
- Конфигурация окружения и провайдеров LLM управляется через `.env`.

### 2. Методология
#### 2.1. Постановка задачи
- Технически это retrieval-augmented question answering (RAG QA) для службы поддержки.
- Вход: вопрос пользователя + история короткого диалога.
- Выход: ответ пользователю или безопасная эскалация на оператора.

#### 2.2. Блок-схема решения (MVP)
1. Telegram update -> FastAPI webhook.
2. Handler получает вопрос и состояние диалога.
3. Retriever ищет релевантные чанки в `core/knowledge`.
4. LLM получает вопрос + контекст + историю.
5. Постобработка ответа: фильтры, проверка уверенности, при необходимости кнопка эскалации.
6. Ответ отправляется пользователю.

#### 2.3. Этапы реализации
- Этап 1. Подготовка данных:
  - входные источники: `.md/.txt` файлы базы знаний;
  - результат: рабочий индекс чанков + сгенерированные стартовые FAQ (`core/generated/faq.json`).
- Этап 2. Базовый inference-контур:
  - интеграция Telegram + FastAPI + LLM client;
  - обработка ошибок внешнего LLM API.
- Этап 3. Контроль качества ответа:
  - порог релевантности retriever;
  - текстовые фильтры и безопасная деградация в сценарий оператора.
- Этап 4. Улучшение качества (следующая итерация):
  - доработка chunking-стратегии;
  - офлайн-набор контрольных вопросов и регресс-тест качества ответов.

### 3. Подготовка пилота
#### 3.1. Способ оценки пилота
- Пилот проводится на ограниченном объеме обращений в Telegram.
- Оцениваются: доля автоматически закрытых обращений, доля корректных эскалаций, техническая стабильность.

#### 3.2. Что считаем успешным пилотом
- Сервис стабильно доступен (healthcheck и обработка webhook/polling).
- Нет критических ошибок генерации в пользовательском диалоге.
- Существенная доля типовых запросов закрывается без участия оператора.

#### 3.3. Подготовка пилота
- Определить список FAQ/knowledge-документов и владельцев их актуальности.
- Зафиксировать набор тестовых вопросов для smoke/regression.
- Согласовать с бизнесом правила эскалации и тон коммуникации.

### 4. Внедрение
#### 4.1. Архитектура решения
- `api`: сборка FastAPI приложения и lifecycle.
- `supportbot`: Telegram handlers и диалоговая логика.
- `mlcore`: retriever и LLM client.
- `core`: settings/runtime/bootstrap/flow.

#### 4.2. Инфраструктура и масштабируемость
- Текущая поставка: один сервис-контейнер (`Dockerfile`, `docker-compose`).
- Горизонтальное масштабирование возможно через stateless API слой; состояние диалога в MVP хранится in-memory и требует выноса во внешнее хранилище на следующем этапе.

#### 4.3. Требования к работе системы
- SLA для MVP: best effort.
- Критично: корректный graceful shutdown, устойчивость к ошибкам LLM API, ответ пользователю при деградации.

#### 4.4. Безопасность системы и данных
- Секреты только в переменных окружения, без коммита в репозиторий.
- В ответах пользователю запрещено раскрытие внутренней архитектуры и служебного контекста.
- Требуется отдельная ревизия по хранению логов и PII перед production-запуском.

#### 4.5. Integration points
- Telegram -> FastAPI webhook endpoint.
- FastAPI/handlers -> LLM provider API (OpenAI-compatible или GigaChat).
- Handlers -> локальная база знаний (`core/knowledge`).

#### 4.6. Риски
- Риск неактуальной базы знаний -> регулярный процесс обновления контента.
- Риск галлюцинаций LLM -> ужесточение промпта, retrieval gating, fallback на оператора.
- Риск роста нагрузки -> переход на внешнее хранилище сессий и кэширование retrieval.

## Открытые вопросы (для следующей итерации)
- Нужна ли интеграция с CRM/тикетингом и на каком этапе.
- Какие метрики качества ответов закрепляем как SLA/SLO.
- Где хранить долгую историю диалогов и как анонимизировать PII.
